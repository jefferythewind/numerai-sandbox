{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "from numerapi import NumerAPI\n",
    "from utils import save_model, load_model, load_model_production, neutralize, get_biggest_change_features, validation_metrics, download_data\n",
    "import numpy as np\n",
    "\n",
    "EXAMPLE_PREDS_COL = \"example_preds\"\n",
    "TARGET_COL = \"target_nomi_20\"\n",
    "\n",
    "downsample_cross_val = 5\n",
    "downsample_full_train = 1\n",
    "\n",
    "cv = 4\n",
    "\n",
    "# std for gaussian noise\n",
    "std = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training_data\n",
      "⠋ Downloading numerai_training_data.parquet⠙ Downloading numerai_training_data.parquet⠹ Downloading numerai_training_data.parquet⠸ Downloading numerai_training_data.parquet⠼ Downloading numerai_training_data.parquet⠴ Downloading numerai_training_data.parquet⠦ Downloading numerai_training_data.parquet⠧ Downloading numerai_training_data.parquet⠇ Downloading numerai_training_data.parquet⠏ Downloading numerai_training_data.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 12:26:44,822 INFO numerapi.utils: target file already exists\n",
      "2022-03-16 12:26:44,824 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Downloading numerai_training_data.parquet\n",
      "reading training data from local file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 12:26:57,731 INFO numexpr.utils: Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-03-16 12:26:57,732 INFO numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# '''I needed to restart kernel'''\n",
    "# training_data = pd.read_parquet('tmp_data.parquet')\n",
    "\n",
    "napi = NumerAPI()\n",
    "\n",
    "print('downloading training_data')\n",
    "download_data(napi, 'numerai_training_data.parquet', 'numerai_training_data.parquet')\n",
    "\n",
    "print(\"reading training data from local file\")\n",
    "training_data = pd.read_parquet('numerai_training_data.parquet').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGETS ['target_nomi_20']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# params we'll use to train all of our models.\n",
    "# Ideal params would be more like 20000, 0.001, 6, 2**6, 0.1, but this is slow enough as it is\n",
    "model_params = {\"n_estimators\": 2000,\n",
    "                \"learning_rate\": .01,\n",
    "                \"max_depth\": 5,\n",
    "                \"num_leaves\": 2 ** 5,\n",
    "                \"colsample_bytree\": 0.1}\n",
    "\n",
    "# pick some targets to use\n",
    "targets = [\"target_nomi_20\"]#[c for c in training_data.columns if c.startswith(\"target_\")]\n",
    "print(\"TARGETS\", targets)\n",
    "\n",
    "# all the possible features to train on\n",
    "feature_cols = [c for c in training_data if c.startswith(\"feature_\")]#\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "'''Preprocess data'''\n",
    "np.save( 'eras.npy', training_data['era'].astype('int').to_numpy() )\n",
    "np.save( 'train_data.npy', training_data[ feature_cols ].to_numpy() )\n",
    "np.save( 'train_targets.npy', training_data[targets].to_numpy() )\n",
    "np.save( 'feature_cols.npy', np.array( feature_cols ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_data.drop( columns=feature_cols, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_data = np.load('train_data.npy', mmap_mode='r+')\n",
    "train_targets = np.load('train_targets.npy', mmap_mode='r+')\n",
    "eras = np.load('eras.npy', mmap_mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def get_time_series_cross_val_splits(all_train_eras, cv = 3, embargo = 12):\n",
    "    len_split = len(all_train_eras) // cv\n",
    "    test_splits = [all_train_eras[i * len_split:(i + 1) * len_split] for i in range(cv)]\n",
    "    # fix the last test split to have all the last eras, in case the number of eras wasn't divisible by cv\n",
    "    test_splits[-1] = np.append(test_splits[-1], all_train_eras[-1])\n",
    "\n",
    "    train_splits = []\n",
    "    for test_split in test_splits:\n",
    "        test_split_max = int(np.max(test_split))\n",
    "        test_split_min = int(np.min(test_split))\n",
    "        # get all of the eras that aren't in the test split\n",
    "        train_split_not_embargoed = [e for e in all_train_eras if not (test_split_min <= int(e) <= test_split_max)]\n",
    "        # embargo the train split so we have no leakage.\n",
    "        # one era is length 5, so we need to embargo by target_length/5 eras.\n",
    "        # To be consistent for all targets, let's embargo everything by 60/5 == 12 eras.\n",
    "        train_split = [e for e in train_split_not_embargoed if abs(int(e) - test_split_max) > embargo and abs(int(e) - test_split_min) > embargo]\n",
    "        train_splits.append(train_split)\n",
    "\n",
    "    # convenient way to iterate over train and test splits\n",
    "    train_test_zip = zip(train_splits, test_splits)\n",
    "    return train_test_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering time series cross validation loop\n",
      "doing split 1 out of 4\n",
      "(2390952,) (2390952,)\n",
      "entering model training loop for split 1\n",
      "model: model_target_nomi_20_original\n",
      "predicting model_target_nomi_20_original\n",
      "model: model_target_nomi_20_with_noise\n",
      "predicting model_target_nomi_20_with_noise\n",
      "model: model_target_nomi_20_both\n",
      "predicting model_target_nomi_20_both\n",
      "doing split 2 out of 4\n",
      "(2390952,) (2390952,)\n",
      "entering model training loop for split 2\n",
      "model: model_target_nomi_20_original\n",
      "predicting model_target_nomi_20_original\n",
      "model: model_target_nomi_20_with_noise\n",
      "predicting model_target_nomi_20_with_noise\n",
      "model: model_target_nomi_20_both\n",
      "predicting model_target_nomi_20_both\n",
      "doing split 3 out of 4\n",
      "(2390952,) (2390952,)\n",
      "entering model training loop for split 3\n",
      "model: model_target_nomi_20_original\n",
      "predicting model_target_nomi_20_original\n",
      "model: model_target_nomi_20_with_noise\n",
      "predicting model_target_nomi_20_with_noise\n",
      "model: model_target_nomi_20_both\n",
      "predicting model_target_nomi_20_both\n",
      "doing split 4 out of 4\n",
      "(2390952,) (2390952,)\n",
      "entering model training loop for split 4\n",
      "model: model_target_nomi_20_original\n",
      "predicting model_target_nomi_20_original\n",
      "model: model_target_nomi_20_with_noise\n",
      "predicting model_target_nomi_20_with_noise\n",
      "model: model_target_nomi_20_both\n",
      "predicting model_target_nomi_20_both\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"entering time series cross validation loop\")\n",
    "\n",
    "train_test_zip = get_time_series_cross_val_splits(np.unique( eras ), cv=cv, embargo=12)\n",
    "\n",
    "ensemble_cols = set()\n",
    "pred_cols = set()\n",
    "\n",
    "predictions = { \"orig\": np.empty(train_data.shape[0]), \"noisy\": np.empty(train_data.shape[0]), \"both\": np.empty(train_data.shape[0]) }\n",
    "\n",
    "for split, train_test_split in enumerate(train_test_zip):\n",
    "    \n",
    "    gc.collect()\n",
    "    print(f\"doing split {split+1} out of {cv}\")\n",
    "    train_split, test_split = train_test_split\n",
    "    train_split_index = np.isin(eras, train_split)\n",
    "    test_split_index = np.isin(eras, test_split)\n",
    "    downsampled_train_split_index = train_split_index\n",
    "    print( train_split_index.shape, test_split_index.shape )\n",
    "\n",
    "    print(f\"entering model training loop for split {split+1}\")\n",
    "    for target in targets:\n",
    "        \n",
    "        \"\"\"original features\"\"\"\n",
    "        \n",
    "        model_name = f\"model_{target}_original\"\n",
    "        print(f\"model: {model_name}\")\n",
    "\n",
    "        split_model = LGBMRegressor(**model_params)\n",
    "        split_model.fit(\n",
    "            train_data[downsampled_train_split_index][::downsample_cross_val],\n",
    "            train_targets[downsampled_train_split_index][::downsample_cross_val].flatten()\n",
    "        )\n",
    "\n",
    "        print(f\"predicting {model_name}\")\n",
    "        predictions['orig'][test_split_index] = split_model.predict(train_data[test_split_index])\n",
    "        \n",
    "        \"\"\"new features\"\"\"\n",
    "        \n",
    "        model_name = f\"model_{target}_with_noise\"\n",
    "        print(f\"model: {model_name}\")\n",
    "\n",
    "        split_model = LGBMRegressor(**model_params)\n",
    "        split_model.fit( \n",
    "            train_data[downsampled_train_split_index][::downsample_cross_val] + np.random.normal(0, std, train_data[downsampled_train_split_index][::downsample_cross_val].shape ) , \n",
    "            train_targets[downsampled_train_split_index][::downsample_cross_val].flatten() \n",
    "        )\n",
    "\n",
    "        print(f\"predicting {model_name}\")\n",
    "        predictions['noisy'][test_split_index] = split_model.predict( train_data[test_split_index] )\n",
    "        \n",
    "        \"\"\"both\"\"\"\n",
    "        \n",
    "        model_name = f\"model_{target}_both\"\n",
    "        print(f\"model: {model_name}\")\n",
    "\n",
    "        split_model = LGBMRegressor(**model_params)\n",
    "        split_model.fit( np.vstack([\n",
    "            train_data[downsampled_train_split_index][::downsample_cross_val],\n",
    "            train_data[downsampled_train_split_index][::downsample_cross_val] + np.random.normal(0, std, train_data[downsampled_train_split_index][::downsample_cross_val].shape )\n",
    "        ]) , np.vstack([\n",
    "            train_targets[downsampled_train_split_index][::downsample_cross_val],\n",
    "            train_targets[downsampled_train_split_index][::downsample_cross_val]\n",
    "        ]).flatten())\n",
    "\n",
    "        print(f\"predicting {model_name}\")\n",
    "        predictions['both'][test_split_index] = split_model.predict( train_data[test_split_index] )\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "''''''\n",
    "# training_data.to_parquet('tmp_data.parquet')\n",
    "training_data['preds_orig'] = predictions['orig']\n",
    "training_data['preds_noisy'] = predictions['noisy']\n",
    "training_data['preds_both'] = predictions['both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>apy</th>\n",
       "      <th>mmc_mean</th>\n",
       "      <th>corr_plus_mmc_sharpe</th>\n",
       "      <th>corr_with_example_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>preds_orig</th>\n",
       "      <td>0.054377</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>1.914835</td>\n",
       "      <td>-0.201473</td>\n",
       "      <td>1215.404782</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>1.846502</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds_noisy</th>\n",
       "      <td>0.050325</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>1.638831</td>\n",
       "      <td>-0.280166</td>\n",
       "      <td>985.666244</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>1.399110</td>\n",
       "      <td>0.810352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds_both</th>\n",
       "      <td>0.055178</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>1.836121</td>\n",
       "      <td>-0.260541</td>\n",
       "      <td>1262.290479</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>1.701947</td>\n",
       "      <td>0.929724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean       std    sharpe  max_drawdown          apy  \\\n",
       "preds_orig   0.054377  0.028398  1.914835     -0.201473  1215.404782   \n",
       "preds_noisy  0.050325  0.030708  1.638831     -0.280166   985.666244   \n",
       "preds_both   0.055178  0.030051  1.836121     -0.260541  1262.290479   \n",
       "\n",
       "             mmc_mean  corr_plus_mmc_sharpe  corr_with_example_preds  \n",
       "preds_orig   0.000064              1.846502                 1.000000  \n",
       "preds_noisy  0.004314              1.399110                 0.810352  \n",
       "preds_both   0.003292              1.701947                 0.929724  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "training_stats = validation_metrics(training_data.dropna(), ['preds_orig', 'preds_noisy', 'preds_both'], example_col=f\"preds_orig\", fast_mode=True)\n",
    "training_stats   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
